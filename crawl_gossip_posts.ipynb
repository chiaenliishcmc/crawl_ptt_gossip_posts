{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9a17f0c-f6d7-4943-8182-bdaec06901af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba9f4913-77b4-42ac-905d-372fc1b1dd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the HTML content of a page\n",
    "def get_html(url):\n",
    "    # Create a session to handle cookies and headers\n",
    "    session = requests.Session()\n",
    "    session.cookies.set('over18', '1')  # Bypass age verification\n",
    "    \n",
    "    # Fetch the content of the page\n",
    "    response = session.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    \n",
    "    else:\n",
    "        print(f'Failed to retrieve the page. Status code: {response.status_code}')\n",
    "\n",
    "# Function to parse the given board page and extract all post titles, dates, and links\n",
    "def parse_board_page(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    #Extract a list of all posts\n",
    "    posts = soup.find_all('div', class_='r-ent')\n",
    "    \n",
    "    post_data = []\n",
    "    for post in posts:\n",
    "        title_tag = post.find('div', class_='title').a\n",
    "        date_tag = post.find('div', class_='date').text.strip()\n",
    "        if title_tag:\n",
    "            title = title_tag.text.strip()\n",
    "            link = 'https://www.ptt.cc' + title_tag['href']\n",
    "            post_data.append((title, link, date_tag))\n",
    "    return post_data, soup\n",
    "\n",
    "# Function to get the link to the previous page\n",
    "def get_previous_page_link(soup):\n",
    "    btn_group = soup.find('div', class_='btn-group btn-group-paging')\n",
    "    prev_link = btn_group.find_all('a')[1]['href']\n",
    "    return 'https://www.ptt.cc' + prev_link\n",
    "\n",
    "# Function to check if the post is within the last 7 days\n",
    "def is_recent(date_str, current_year):\n",
    "    post_date = datetime.strptime(f\"{current_year}/{date_str}\", '%Y/%m/%d')\n",
    "    return post_date >= datetime.now() - timedelta(days=7)\n",
    "\n",
    "# Function to save the data to a CSV file\n",
    "def save_to_csv(data, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['標題', 'URL'])\n",
    "        writer.writerows([(title, url) for title, url, _ in data])\n",
    "\n",
    "\n",
    "# Function to read the CSV file and extract links\n",
    "def read_csv_and_get_links(filename):\n",
    "    links = []\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip the header row\n",
    "        for row in reader:\n",
    "            if len(row) > 1:  # Ensure the row has enough columns\n",
    "                links.append(row[1])  # The URL is in the second column\n",
    "    return links\n",
    "\n",
    "#----------------Functions for crawling each post page-------------------------#\n",
    "\n",
    "# Cleans up post content\n",
    "def clean_text(text):\n",
    "    # Remove the footer starting from '--' or '--※'\n",
    "    cleaned_text = re.split(r'\\n--\\n|--※', text)[0]\n",
    "    \n",
    "    # Replace multiple newlines with a single newline\n",
    "    cleaned_text = re.sub(r'\\n+', '\\n', cleaned_text).strip()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "#Extracts post content (text only)\n",
    "def get_post_content(main_content):\n",
    "\n",
    "    # Remove all 'span' tags with meta class (author, board, title, date, and other meta information)\n",
    "    for span in main_content.find_all('span', class_='article-meta-value'):\n",
    "        span.decompose()\n",
    "    \n",
    "    for span in main_content.find_all('span', class_='article-meta-tag'):\n",
    "        span.decompose()\n",
    "    \n",
    "    # Remove other unnecessary tags if present (e.g., 'div' with class 'push')\n",
    "    for div in main_content.find_all('div', class_='push'):\n",
    "        div.decompose()\n",
    "    \n",
    "    # The main content is now the text within the 'main-content' div\n",
    "    post_text = main_content.text.strip()\n",
    "    \n",
    "    return clean_text(post_text)\n",
    "\n",
    "#----Extract specific content from each comment -------#\n",
    "\n",
    "#Get comment content\n",
    "def get_content(comment):\n",
    "    try:\n",
    "        content = comment.find('span', class_='f3 push-content').text.split(\": \")[1]\n",
    "        # Check subsequent siblings for additional comment lines\n",
    "        next_sibling = comment.find_next_sibling()\n",
    "        while next_sibling and not next_sibling.get('class') == ['push']:\n",
    "            if next_sibling.name == 'span' and 'f3' in next_sibling.get('class', []):\n",
    "                content += next_sibling.text.strip()\n",
    "            next_sibling = next_sibling.find_next_sibling()\n",
    "    except:\n",
    "        content = \"\"\n",
    "    \n",
    "    return content\n",
    "\n",
    "#Get comment author\n",
    "def get_user(comment):\n",
    "    try:\n",
    "        user = comment.find('span', class_='f3 hl push-userid').text\n",
    "    except:\n",
    "        user = \"\"\n",
    "    return user\n",
    "\n",
    "#Get comment date time\n",
    "def get_datetime(comment):\n",
    "       #Get date and time\n",
    "        ip_date_time_sec = comment.find('span', class_='push-ipdatetime')\n",
    "        ip_date_time = comment.find('span', class_='push-ipdatetime').text.strip()\n",
    "        \n",
    "        if ip_date_time:\n",
    "            # Check subsequent siblings for additional comment lines\n",
    "            next_sibling = ip_date_time_sec.next_element.next_element\n",
    "            while next_sibling and next_sibling.name is None:\n",
    "                ip_date_time += next_sibling.strip()\n",
    "                next_sibling = next_sibling.next_element\n",
    "                \n",
    "        #If date and time exist outside the span (on a new line)\n",
    "        else:\n",
    "            next_sibling = ip_date_time_sec.next_element\n",
    "\n",
    "            while (next_sibling and ( (next_sibling.name is None) or (next_sibling.name == 'span' and next_sibling.get('class') == ['f3']))):\n",
    "                ip_date_time = next_sibling\n",
    "                next_sibling = next_sibling.next_element\n",
    "        \n",
    "        ip_date_time = [item for item in ip_date_time.split(\" \") if item!=\"\"]\n",
    "\n",
    "        return ip_date_time\n",
    "\n",
    "#-----------------------------------------#\n",
    "\n",
    "#Functions to extract comments (author, content, time)\n",
    "def get_comments(main_content):\n",
    "    \n",
    "    # Remove the post metadata (post author, title, etc.)\n",
    "    metas = main_content.find_all('div', class_='article-metaline')\n",
    "    for meta in metas:\n",
    "        meta.extract()\n",
    "    metas_right = main_content.find_all('div', class_='article-metaline-right')\n",
    "    for meta in metas_right:\n",
    "        meta.extract()\n",
    "    \n",
    "    # Remove the IP address information\n",
    "    ip_info = main_content.find('span', class_='f2')\n",
    "    if ip_info:\n",
    "        ip_info.extract()\n",
    "\n",
    "    \n",
    "    # Extract comments \n",
    "    comments = main_content.find_all('div', class_='push') #List of all comment blocks\n",
    "    comments = [comment for comment in comments if comment.get(\"class\")==['push']] #Filter out element whose class isn't identical to 'push' (non-comment)\n",
    "    comments_data = []\n",
    "    \n",
    "    for comment in comments:\n",
    "\n",
    "        #Get user and content\n",
    "        user = get_user(comment)\n",
    "        content = get_content(comment)\n",
    "        ip_date_time = get_datetime(comment)\n",
    "        \n",
    "        try:\n",
    "            date = ip_date_time[1]\n",
    "            time = ip_date_time[2].split(\"\\n\")[0]\n",
    "            date_time = \" \".join([date,time])\n",
    "        except:\n",
    "            print(\"No Date time\")\n",
    "            date_time = \"\"\n",
    "        comments_data.append((user, date_time, content))\n",
    "\n",
    "        #print(f'User: {user}, Comment: {content}, Time: {date} {time}')\n",
    "    return comments_data\n",
    "\n",
    "#Functions to get post data and all comment data\n",
    "def get_post (url):\n",
    "    post_html = get_html(url) \n",
    "    soup = BeautifulSoup(post_html, 'html.parser')\n",
    "    post_data = {}\n",
    "\n",
    "    #Get relevant info of the post (author, title, time)\n",
    "    post_data[\"作者\"], category, post_data[\"標題\"], post_data[\"時間\"] = map(lambda x: x.text.strip(),soup.find_all(\"span\", class_= \"article-meta-value\"))\n",
    "\n",
    "    #Extract main content \n",
    "    main_content = soup.find('div', id='main-content')\n",
    "\n",
    "    #Get comment data from main content\n",
    "    comment_data = get_comments(main_content)\n",
    "    #Get post text from main content\n",
    "    post_data[\"內文\"] = get_post_content(main_content)\n",
    "\n",
    "    return post_data, comment_data \n",
    "    #Returns a dictionary with 4 keys 作者、標題、時間、內文 and a list of tuples for comments (one tuple for each comment, formatted (作者,時間,留言內文）)\n",
    "\n",
    "\n",
    "#Save post data to txt file\n",
    "def save_to_txt (post_content, comment_content, index):\n",
    "    \n",
    "    if not os.path.exists('gossip_posts'):\n",
    "        os.makedirs('gossip_posts')\n",
    "\n",
    "    # Format the post content\n",
    "    post_text = (f\"作者: {post_content['作者']}\\n\"\n",
    "                 f\"標題: {post_content['標題']}\\n\"\n",
    "                 f\"時間: {post_content['時間']}\\n\\n\"\n",
    "                 f\"{post_content['內文']}\\n\")\n",
    "    \n",
    "    # Format the comments\n",
    "    comments_text = \"\\n\\n留言:\\n\\n\"\n",
    "    for comment in comment_content:\n",
    "        comments_text += f\"作者: {comment[0]}\\n時間: {comment[1]}\\n內容: {comment[2]}\\n\\n\"\n",
    "    \n",
    "    # Combine post content and comments\n",
    "    full_text = post_text + comments_text\n",
    "    \n",
    "    # Write to a text file\n",
    "    with open(f'gossip_posts/gossip_post_{index}.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(full_text)\n",
    "    \n",
    "    print(f\"gossip_post_{index}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf91153d-6033-443c-bcf4-507fd11f5ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # URL of the PTT Gossiping board (i.e where the newest gossiping posts are)\n",
    "    base_url = 'https://www.ptt.cc/bbs/Gossiping/index.html'\n",
    "    \n",
    "    all_posts = []\n",
    "    current_url = base_url\n",
    "    \n",
    "    # Get the current year\n",
    "    current_year = datetime.now().year\n",
    "    \n",
    "    while True:\n",
    "        # Get the HTML content of the current page\n",
    "        html_content = get_html(current_url)\n",
    "        \n",
    "        # Parse the board page and extract post data\n",
    "        post_data, soup = parse_board_page(html_content)\n",
    "        \n",
    "        # Filter posts from the last 7 days\n",
    "        recent_posts = [post for post in post_data if is_recent(post[2], current_year)]\n",
    "        \n",
    "        # If no recent posts are found, break the loop, no need to continue to prev page\n",
    "        if not recent_posts:\n",
    "            break\n",
    "        \n",
    "        # Add the recent posts to the list of all posts\n",
    "        all_posts.extend(recent_posts)\n",
    "        \n",
    "        # Get the link to the previous page\n",
    "        current_url = get_previous_page_link(soup)\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    csv_filename = 'ptt_gossiping_recent_posts.csv'\n",
    "    save_to_csv(all_posts, csv_filename)\n",
    "    \n",
    "    print(f\"Data saved to {csv_filename}\")\n",
    "\n",
    "    #--------------------------------------------------------------#\n",
    "    \n",
    "    # Get links of posts from last 7 days to crawl\n",
    "    csv_filename = 'ptt_gossiping_recent_posts.csv'\n",
    "    links = read_csv_and_get_links(csv_filename)\n",
    "\n",
    "    problem_links = []\n",
    "    link_index = 0\n",
    "    \n",
    "    #Crawl each post and save data to txt file\n",
    "    for link in links:\n",
    "        try:\n",
    "            post, comment = get_post(link)\n",
    "            print (f\"Retrieved {link_index}\")\n",
    "            save_to_txt(post, comment, link_index)\n",
    "        except:\n",
    "            print(f\"Problem with {link_index}\")\n",
    "            problem_links.append(link)\n",
    "        link_index += 1\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
